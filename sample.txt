mlp_param_list_3 = []
mlp_mse_list_3 = []

mlp_param_list_2 = []
mlp_mse_list_2 = []

mlp_param_list_4 = []
mlp_mse_list_4 = []

kan_param_list = []
kan_mse_list = []

kan_train_time = []
mlp_train_time_3 = []
mlp_train_time_2 = []
mlp_train_time_4 = []

hidden_sizes = [ 8, 16, 32, 48, 64]
# widths_list = [[1, 4, 1], [1, 2, 2, 1], [1, 4, 4, 1], [1, 8, 8, 1], [1, 16, 16, 1]]

widths_list = [[1,2,2,1],[1,4,4,1],[1,6,6,1],[1,8,8,1],[1,12,12,1]]

dataset = create_dataset(f1, n_var=1, ranges=[-1, 1],train_num=3000) #change f1 to other function names for testing on desired function for graphs

x_train = dataset['train_input']
y_train = dataset['train_label']
x_test = dataset['test_input']
y_test = dataset['test_label']

train_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=32, shuffle=True)
test_loader = DataLoader(TensorDataset(x_test, y_test), batch_size=128)

for h in hidden_sizes:
    # === MLP ===
    mlp_model = MLP_3(input_dim=1, hidden_dim=h, output_dim=1)
    optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.001)
    loss_fn = nn.MSELoss()

    start = time.time()
    for epoch in range(50):
        mlp_model.train()
        for xb, yb in train_loader:
            pred = mlp_model(xb)
            loss = loss_fn(pred, yb)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    end = time.time()

    with torch.no_grad():
        mlp_pred = mlp_model(x_test)
        mlp_mse = F.mse_loss(mlp_pred, y_test).item()

    count= 0
    for p in mlp_model.parameters():
      if p.requires_grad:
        p_count = p.numel()
        count += p_count
    mlp_param_list_3.append(count)
    mlp_mse_list_3.append(mlp_mse)
    mlp_train_time_3.append(end-start)
    # === MLP ===
    mlp_model = MLP_2(input_dim=1, hidden_dim=h, output_dim=1)
    optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.001)
    loss_fn = nn.MSELoss()
    start = time.time()
    for epoch in range(50):
        mlp_model.train()
        for xb, yb in train_loader:
            pred = mlp_model(xb)
            loss = loss_fn(pred, yb)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    end = time.time()
    with torch.no_grad():
        mlp_pred = mlp_model(x_test)
        mlp_mse = F.mse_loss(mlp_pred, y_test).item()
    count = 0
    for p in mlp_model.parameters():
      if p.requires_grad:
        p_count = p.numel()
        count += p_count
    mlp_param_list_2.append(count)
    mlp_mse_list_2.append(mlp_mse)
    mlp_train_time_2.append(end-start)
    # === MLP ===
    mlp_model = MLP_4(input_dim=1, hidden_dim=h, output_dim=1)
    optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.001)
    loss_fn = nn.MSELoss()
    start = time.time()
    for epoch in range(50):
        mlp_model.train()
        for xb, yb in train_loader:
            pred = mlp_model(xb)
            loss = loss_fn(pred, yb)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    end = time.time()
    with torch.no_grad():
        mlp_pred = mlp_model(x_test)
        mlp_mse = F.mse_loss(mlp_pred, y_test).item()
    count = 0
    for p in mlp_model.parameters():
      if p.requires_grad:
        p_count = p.numel()
        count += p_count
    mlp_param_list_4.append(count)
    mlp_mse_list_4.append(mlp_mse)
    mlp_train_time_4.append(end-start)

for width in widths_list:
    # === KAN ===
    kan_model = KAN(width=width)
    start = time.time()
    kan_model.fit(dataset, steps=50,opt='adam', lr=0.001)
    end = time.time()

    with torch.no_grad():
        kan_pred = kan_model(x_test)
        kan_mse = F.mse_loss(kan_pred, y_test).item()

    count = 0
    for p in kan_model.parameters():
        if p.requires_grad:
            p_count = p.numel()
            count += p_count
    kan_param_list.append(count)
    kan_mse_list.append(kan_mse)
    kan_train_time.append(end-start)


